---
title: "HW2"
author: "Cohort 2, Group 7 - Hyeuk Jung, Jiaqi Li, Xichen Luo, Huanyu Liu"
date: "1/26/2019"
output: pdf_document
---

# Problem 1

(a)
```{r, eval=TRUE}
set.seed(1)
b0 = 0
b1.1 = 1.1
b2.1 = -0.25
WN1 = rnorm(1000, mean = 0, sd = 1)
MySeq1 = rnorm(2)
for (i in 3:1000){
  MySeq1 = append(MySeq1, b0+b1.1*MySeq1[i-1]+b2.1*MySeq1[i-2]+WN1[i])
}

plot(MySeq1, type = "l")
acf(MySeq1, lag.max = 20)
```
  

(b)
$$\begin{aligned}
1 - \phi_1 x - \phi_2 x^2 &= 0\\
1 - 1.1x + 0.25x^2 &= 0\\
x_1 = \frac{1.1 + \sqrt{1.1^2 - 4 \times 0.25 \times 1}}{2 \times 0.25} &\approx 3.12\\
\omega_1 = x_1^{-1} = 3.12^{-1} = 0.3209\\
x_2 = \frac{1.1 - \sqrt{1.1^2 - 4 \times 0.25 \times 1}}{2 \times 0.25} &\approx 1.28\\
\omega_2 = x_2^{-1} = 0.7791\\
\end{aligned}$$  
Both of the two different characteristic roots modulus of the above polynomial are less than 1, so this AR(2) process is stationary.    
(c)

$$\begin{aligned}
\mu &= \phi_0 = 2\\
\text{Let }X_t &= r_t - \mu\\
X_{t} &= \phi_1 X_{t-1} + \phi_2 X_{t-2} + \epsilon_t\\
\end{aligned}$$


$$\begin{aligned}
\mu &= \phi_0 = 2\\
\text{Let }X_t &= r_t - \mu\\
X_{t} &= \phi_1 X_{t-1} + \phi_2 X_{t-2} + \epsilon_t\\
X_{t+6} &= \phi_1 X_{t+5} + \phi_2 X_{t+4} + \epsilon_{t+6}\\
&= \phi_1 (\phi_1 X_{t+4} + \phi_2 X_{t+3} + \epsilon_{t+5}) + \\ 
   &\phi_2 (\phi_1 X_{t+3} + \phi_2 X_{t+2} + \epsilon_{t+4}) + \epsilon_{t+6}\\
& = \phi_1^2 X_{t+4} + 2\phi_1 \phi_2 X_{t+3} + \phi_2^2 X_{t+2} +\\
&\phi_1 \epsilon_{t+5} + \phi_2 \epsilon_{t+4} + \epsilon_{t+6}\\
& \dots\\
&= \dots + (\phi_2^3 + 6\phi_1^2 \phi_2^2 + 5\phi_1^4\phi_2^1 + \phi_1^6)\epsilon_t\\
&\therefore \frac{\partial[r_{t+6}-\mu]}{\partial \epsilon_t} = \phi_2^3 + 6\phi_1^2 \phi_2^2 + 5\phi_1^4\phi_2^1 + \phi_1^6\\
& \approx 0.37956\\
\end{aligned}$$  

(d)

$$\begin{aligned}
1 - 0.9x - 0.8x^2 &= 0\\
x_1 = \frac{0.9 + \sqrt{0.9^2 + 4 \times 0.8 \times 1}}{2 \times -0.8} &\approx -1.814\\
\omega_1 = x_1^{-1} = (-1.8141)^{-1} &= -0.5512\\
x_2 = \frac{0.9 - \sqrt{0.9^2 + 4 \times 0.8 \times 1}}{2 \times -0.8} &\approx 0.689\\
\omega_2 = x_2^{-1} &= 1.4512\\
\end{aligned}$$  
The modulus of one of the two different characteristic roots of the above polynomial is larger than 1, so this AR(2) process is not stationary.  
For the dynamic multiplier, the calculation is similar as in (c).  
$$\begin{aligned}
\frac{\partial[r_{t+6}-\mu]}{\partial \epsilon_t} &= \phi_2^3 + 6\phi_1^2 \phi_2^2 + 5\phi_1^4\phi_2^1 + \phi_1^6\\
& \approx 6.77824\\
\end{aligned}$$



# Problem 2

## Q1.
```{r, eval=TRUE}
PPI = read.csv("PPIFGS.csv")
valPPI = PPI$VALUE
delPPI = diff(valPPI)
logPPI = log(valPPI)
dellogPPI = diff(log(valPPI))
par(mfrow = c(2,2))
plot(valPPI, type = "l", ylab = "value")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted", lwd = par("lwd"), equilogs = TRUE)
plot(logPPI, type = "l", ylab = "log value")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted", lwd = par("lwd"), equilogs = TRUE)
plot(delPPI, type = "l", ylab = "change in value")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted", lwd = par("lwd"), equilogs = TRUE)
plot(dellogPPI, type = "l", ylab = "change in log value")
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted", lwd = par("lwd"), equilogs = TRUE)
```

## Q2.

$\Delta logPPI$ looks covariance-stationary. 

Among these four versions, the plot PPI in levels, and the plot Log PPI, show an uptrending. In that case, these two series are not covariance-stationary.

Compared the plot, PPI Difference, and the plot, Log PPI Difference, their fluctations don't show an uptrending or downtrending, while the volatility of the second plot, Log PPI Difference, shows more constant. In that case, the plot, Log PPI Difference, is covariance-stationary.


## Q3. 
```{r,eval=TRUE}
par(mfrow = c(1,1))
acf(dellogPPI, lag.max = 12)
```

Since the plot of ACF converges very quickly to zero, it means that as time passing, in the long run, the lags could be less likely to have an effect on Log PPI Difference.

If the ACF converges very slowly, it means that a lot of short-term lags could affect the Log PPI Difference. In that case, the volatility of Log PPI Difference could be related to time instead of a constant.

## Q4. 
```{r, eval=TRUE}
par(mfrow = c(1,1))
pacf(dellogPPI, lag.max = 12)
```

Since the Partial ACF shows significance of lag 1, 2, 3, and 11 compared to other lags. In that case, they should be included in the regression analysis.
Therefore, we can consider AR(1), AR(2), AR(3), and AR(11) could better explain the changing of series y. 

## Q5.

(a)
```{r,eval=TRUE}
n = length(dellogPPI)
yt = dellogPPI

yt.lag1 = yt[1:(n-1)]
Y1 = yt[2:n]

yt.lag1 = yt[1:(n-2)]
yt.lag2 = yt[2:(n-1)]
Y2 = yt[3:n]

yt.lag1 = yt[1:(n-3)]
yt.lag2 = yt[2:(n-2)]
yt.lag3 = yt[3:(n-1)]
Y3 = yt[4:n]

ar3 = lm(Y3~yt.lag1+yt.lag2+yt.lag3)

for (i in 1:11){
  assign(paste("yt.lag", i, sep = ""),
         yt[i:(n-(12-i))])
}
Y11 = yt[12:n]

ar11 = lm(Y11~yt.lag1+yt.lag2+yt.lag3+
            yt.lag4+yt.lag5+yt.lag6+
            yt.lag7+yt.lag8+yt.lag9+
            yt.lag10+yt.lag11)

ar3.coef = summary(ar3)
ar11.coef = summary(ar11)

1/polyroot(c(1,-ar3.coef$coefficients[2,1],
           -ar3.coef$coefficients[3,1],
           -ar3.coef$coefficients[4,1]))

1/polyroot(c(11,-ar11.coef$coefficients[2,1],
           -ar11.coef$coefficients[3,1],
           -ar11.coef$coefficients[4,1],
           -ar11.coef$coefficients[5,1],
           -ar11.coef$coefficients[6,1],
           -ar11.coef$coefficients[7,1],
           -ar11.coef$coefficients[8,1],
           -ar11.coef$coefficients[9,1],
           -ar11.coef$coefficients[10,1],
           -ar11.coef$coefficients[11,1],
           -ar11.coef$coefficients[12,1]))

```
   
Stationary test for AR(3): Using the polyroot function, we got three characteristic roots: 0.7805586, -0.3207432-0.4890206i, and -0.3207432+0.4890206i. 
For the imaginary results, we checked if they are less than one in modulus.
\begin{align*}
|\omega_1| &= 0.7805586 \\
|\omega_2| &= \sqrt{(-0.3207432)^2 + (-0.4890206)^2} = \sqrt{0.3420} \approx 0.5848 \\
|\omega_3| &= \sqrt{(-0.3207432)^2 + (0.4890206)^2} = \sqrt{0.3420} \approx 0.5848 \\
\end{align*} 
As all characteristic roots are less than one, we can conclude that the model is stationary.

Stationary test for AR(11):
\begin{align*}
|\omega_1| &= 0.7460541 \\
|\omega_2| \text{ and } |\omega_3| &= \sqrt{(0.6037606)^2 + (\pm 0.4191605)^2} \approx 0.7350 \\
|\omega_4| \text{ and } |\omega_5| &= \sqrt{(-0.6622935)^2 + (\pm 0.2021894)^2} \approx 0.6925 \\
|\omega_6| \text{ and } |\omega_7| &= \sqrt{(-0.4375012)^2 + (\pm 0.5084891)^2} \approx 0.6708 \\
|\omega_8| \text{ and } |\omega_9| &= \sqrt{(0.2706054)^2 + (\pm 0.6636678)^2} \approx 0.7167 \\
|\omega_{10}| \text{ and } |\omega_{11}| &= \sqrt{(-0.1399119)^2 + (\pm 0.6679610)^2} \approx 0.6825 \\
\end{align*} 
As all characteristic roots are less than one, we can conclude that the model is stationary.

(b)
```{r,eval=TRUE}
par(mfrow = c(2,2))
plot(ar3$residuals, type = "l")
plot(ar11$residuals, type = "l")

par(mfrow = c(2,2))
acf(ar3$residuals)
acf(ar11$residuals)
```
  
(c)
```{r,eval=TRUE}

Box.test(ar3$residuals, lag = 8, type = "Ljung-Box")
Box.test(ar3$residuals, lag = 12, type = "Ljung-Box")

Box.test(ar11$residuals, lag = 8, type = "Ljung-Box")
Box.test(ar11$residuals, lag = 12, type = "Ljung-Box")

aic = c(AIC(ar3),AIC(ar11))
bic = c(BIC(ar3),BIC(ar11))
aic
bic
```

As all characteristic roots are less than one, we can conclude that the model is stationary.

|       |Box-Ljung (lag = 8)   |Box-Ljung (lag = 12) |AIC       |BIC      |
|:-----:|:--------------------:|:-------------------:|:--------:|--------:|
|AR(3)  |5.6426 (p-val:0.6872) |11.845 (p-val:0.4582)|-1624.454 |-1606.462|
|AR(11) |0.95008 (p-val:0.9985)|2.9547 (p-val:0.9959)|-1576.268 |-1529.879|

From the test results, we prefer the AR(3) model, which has smaller AIC and BIC value.


##6.
```{r,eval=TRUE}
library(xts)
PPI = read.csv("PPIFGS.csv")
date = as.Date(as.vector(PPI$DATE))
PPI.xts = xts(PPI$VALUE, order.by = date)
PPI.sample = diff(log(as.vector(PPI.xts["/2005-10-01"])))
PPI.pred = diff(log(as.vector(PPI.xts["2005-10-01/"])))

ar3.test = ar(PPI.sample,order.max = 3, aic = F)
ar11.test = ar(PPI.sample,order.max = 11, aic = F)

n.sample = length(PPI.sample)
n.pred = length(PPI.pred)

pred3 = predict(ar3.test, n.ahead = 39)$pred
pred11 = predict(ar11.test, n.ahead = 39)$pred


v3 = sum((PPI.pred-pred3)^2)/n.pred
v11 = sum((PPI.pred-pred11)^2)/n.pred
c(v3,v11)

#rw---------------------
rw.sd = sqrt(var(PPI.sample))
monte = vector()
for (i in 1:1000)
{
  set.seed(i)
  rw.test = PPI.sample[n.sample]
  for (i in 1:n.pred){
    rw.test = append(rw.test, rw.test[i]+rnorm(1,0,rw.sd))
  }
  rw.v = sum((PPI.pred-rw.test[-1])^2)/n.pred
  monte = append(monte,rw.v)
}
mean(monte)
```

MSPE result:

|       |Random walk |AR(3)        |AR(11)       |
|:-----:|:----------:|:-----------:|:-----------:|
|MSPE   |0.003254651 |0.0003385095 |0.0003406913 |

Based on the mean squared prediction error of each model, we conclude that the AR(3) model predicts the data better than other models.

